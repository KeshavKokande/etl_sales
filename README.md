# üöÄ MY\_DATA\_PIPELINE

A complete end-to-end data engineering project using **Airflow**, **dbt**, **Snowflake**, and **Docker**. This repository automates the ingestion, transformation, and testing of cleaned e-commerce order data.

---

## üßæ Table of Contents

* [Overview](#overview)
* [Project Architecture](#project-architecture)
* [Tech Stack](#tech-stack)
* [Folder Structure](#folder-structure)
* [Setup Instructions](#setup-instructions)
* [Running the Pipeline](#running-the-pipeline)
* [Airflow DAG Details](#airflow-dag-details)
* [dbt Model Structure](#dbt-model-structure)
* [Incremental Load Logic](#incremental-load-logic)
* [CI/CD Planning](#ci-cd-planning)

---

## üîç Overview

This project loads raw e-commerce data (CSV), cleans it, and ingests it into **Snowflake**. It then runs **dbt models** to transform the data into meaningful **DIM** and **FACT** tables. The entire process is orchestrated using **Apache Airflow**, containerized using **Docker**.

---

## üèóÔ∏è Project Architecture

```
+--------------+       +----------------+       +----------------+       +-------------+
| MOCK_DATA.CSV|  -->  | Cleaned CSV    |  -->  | Snowflake Table|  -->  | dbt Models  |
+--------------+       +----------------+       +----------------+       +-------------+
                                                              
                      (Airflow orchestrates the full pipeline)
```

---

## ‚öôÔ∏è Tech Stack

* **Apache Airflow**: Workflow orchestration
* **dbt (Data Build Tool)**: Data transformation
* **Snowflake**: Cloud data warehouse
* **Docker**: Containerization and environment setup
* **Python**: Data cleaning and transformation

---

## üóÇÔ∏è Folder Structure

```bash
MY_DATA_PIPELINE/
|‚îú‚îÄ‚îÄ Airflow/
|   |
|   ‚îú‚îÄ‚îÄ dags/               # Airflow DAGs
|   ‚îú‚îÄ‚îÄ airflow.cfg         # Airflow config
|   ‚îú‚îÄ‚îÄ .env                # Snowflake credentials
|   ‚îú‚îÄ‚îÄ docker-compose.yml   # Compose file to run Airflow + dbt
|   ‚îú‚îÄ‚îÄ dockerfile          # Dockerfile for Airflow image
|
‚îú‚îÄ‚îÄ dbt_project/
|   ‚îú‚îÄ‚îÄ mockkaro_dbt/
|       ‚îú‚îÄ‚îÄ models/         # dbt models (staging, marts)
|       ‚îú‚îÄ‚îÄ snapshots/      # dbt snapshots (optional)
|       ‚îú‚îÄ‚îÄ seeds/          # Seed data if needed
|       ‚îú‚îÄ‚îÄ tests/          # Custom tests
|       ‚îú‚îÄ‚îÄ dbt_project.yml  # dbt config file
|
‚îî‚îÄ‚îÄ MOCK_DATA.csv           # Input dataset
```

---

## üõ†Ô∏è Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/MY_DATA_PIPELINE.git
cd MY_DATA_PIPELINE
```

### 2. Set Snowflake Credentials

Inside `Airflow/.env`, configure:

```dotenv
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_db
SNOWFLAKE_SCHEMA=RAW
```

### 3. Start the Airflow + dbt Environment

```bash
cd Airflow
docker-compose up --build
```

Open Airflow UI: [http://localhost:8080](http://localhost:8080)

---

## ‚ñ∂Ô∏è Running the Pipeline

Once Airflow is running, enable the DAG `combined_pipeline_dag`:

**Task Flow:**

```
print_welcome ‚Üí print_date ‚Üí clean_data ‚Üí load_to_snowflake
         ‚Üí dbt_tasks (run_stg ‚Üí run_marts ‚Üí test_marts) ‚Üí print_random_quote
```

---

## üß† Airflow DAG Details

**DAG Name:** `combined_pipeline_dag`

**Key Operators:**

* `PythonOperator`: for CSV cleaning, Snowflake loading, quote generation
* `BashOperator`: for dbt execution (`dbt run`, `dbt test`)
* `TaskGroup`: groups dbt run/test together

---

## üìä dbt Model Structure

* `staging/`: Extract columns from raw

  * `stg_orders.sql`, `stg_customers.sql` etc.
* `marts/dim/`: Cleaned dimension tables

  * `dim_product.sql`, `dim_store.sql` etc.
* `marts/fact/`: Aggregated fact tables

  * `fact_orders.sql`, `fact_sales_summary.sql`

**Schema Configuration:** `dbt_project.yml` handles folder-specific schema overrides.

---

## üìà Incremental Load Logic (Planned or In Use)

* For staging tables: `materialized='incremental'`
* Add `unique_key` in `config` block
* Add logic like:

```sql
WHERE {{ this._is_incremental() }} AND updated_at > (SELECT MAX(updated_at) FROM {{ this }})
```

---

## üöÄ CI/CD Planning (Optional Setup)

**To integrate GitHub Actions later:**

* Lint Python files with `flake8`
* Run `dbt build` in container
* Deploy using GitHub Secrets for Snowflake

**Sample GitHub Action YAML:** *(add later)*

```yaml
on: [push]
jobs:
  dbt-pipeline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: docker-compose -f Airflow/docker-compose.yml up --build
```

---

## üôã FAQs

* **Q:** Can I run dbt models outside Airflow?
  **A:** Yes, run them manually using:

  ```bash
  cd dbt_project/mockkaro_dbt
  dbt run --select marts
  ```

* **Q:** How do I check logs?
  **A:** Inside Airflow UI > DAG Runs > Task Logs

* **Q:** What if cleaned data loads duplicate rows?
  **A:** Add deduplication logic or switch to incremental loading strategy.

---

## ü§ù Contributions

Feel free to raise issues, PRs, or suggest improvements.

---

Made with ‚ù§Ô∏è by Keshav Kokande
